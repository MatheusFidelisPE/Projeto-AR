{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatheusFidelisPE/Projeto-AR/blob/main/projeto_1va_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeJ3wCaKe2Wl"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap06/cap06-main.ipynb)\n",
        "\n",
        "# Tema 8 - Estudo de parâmetros com ambientes diversos\n",
        "\n",
        "Os algoritmos de aprendizagem possuem uma característica interessante, que é a capacidade de mudança de uma execução para outra após alterar minimamente um parâmetro do modelo. Assim, o trabalho atual busca testar diferentes valores para importantes parâmetros ** epsilon, número de passos, gamma, alfa e taxa de aprendizagem**.\n",
        "\n",
        "Treinaremos o agente por **N** episódios, após isso faremos a execução de **M** episódios focando na recompensa obtida durante a execução dos M episódios. Para reduzir reduzir o fator aleatório, faremos a execução dos M episódios **X** vezes para calcular a média dessas recompensas e o desvio padrão.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAHITU7VhsM7"
      },
      "source": [
        "## Configurações Iniciais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NS23BU8R1vq-",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    # for saving videos\n",
        "    !apt-get install ffmpeg\n",
        "\n",
        "    !pip install gymnasium moviepy\n",
        "    !pip install optuna\n",
        "\n",
        "    # clone repository\n",
        "    !git clone https://github.com/pablo-sampaio/rl_facil\n",
        "    sys.path.append(\"/content/rl_facil\")\n",
        "\n",
        "else:\n",
        "    from os import path\n",
        "    sys.path.append( path.dirname( path.dirname( path.abspath(\"__main__\") ) ) )\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0Gzf7VhkiHxQ"
      },
      "outputs": [],
      "source": [
        "# Imports importantes para criação dos algoritmos, exeução e apresentação dos gráficos e outros valores.\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import TimeLimit\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from cap06.nstep_sarsa import run_nstep_sarsa\n",
        "\n",
        "from envs import RacetrackEnv\n",
        "from envs.wrappers import ObservationDiscretizerWrapper\n",
        "\n",
        "from util.experiments import repeated_exec\n",
        "from util.plot import plot_result, plot_multiple_results\n",
        "from util.notebook import display_videos_from_path\n",
        "from util.qtable_helper import evaluate_qtable_policy, record_video_qtable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sarsa personalizado para treinar e testar"
      ],
      "metadata": {
        "id": "QLShaRwisckN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from util.qtable_helper import epsilon_greedy\n",
        "from collections import deque\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "def run_nstep_sarsa_by_eps(env, episodes, nsteps=1, lr=0.1, gamma=0.95, epsilon=0.1, verbose=False,**kargs):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "    assert isinstance(nsteps, int)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # inicializa a tabela Q com valores aleatórios pequenos (para evitar empates)\n",
        "    if kargs.get(\"Q\", None) is None:\n",
        "      Q = np.random.uniform(low=-0.01, high=+0.01, size=(env.observation_space.n, num_actions))\n",
        "    # Em caso de teste, a tabela Q já é carregada\n",
        "    else:\n",
        "      Q = kargs[\"Q\"].copy()\n",
        "    gamma_array = np.array([ gamma**i for i in range(0,nsteps)])\n",
        "    gamma_power_nstep = gamma**nsteps\n",
        "\n",
        "    # para cada episódio, guarda sua soma de recompensas (retorno não-descontado)\n",
        "    sum_rewards_per_ep = []\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "\n",
        "        state, _ = env.reset()\n",
        "        # escolhe a próxima ação\n",
        "        action = epsilon_greedy(Q, state, epsilon)\n",
        "\n",
        "        # históricos de: estados, ações e recompensas\n",
        "        hs = deque(maxlen=nsteps)\n",
        "        ha = deque(maxlen=nsteps)\n",
        "        hr = deque(maxlen=nsteps)\n",
        "\n",
        "        # executa 1 episódio completo, fazendo atualizações na Q-table\n",
        "        while not done:\n",
        "            # realiza a ação\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            sum_rewards += reward\n",
        "\n",
        "            # escolhe (antecipadamente) a ação do próximo estado\n",
        "            next_action = epsilon_greedy(Q, next_state, epsilon)\n",
        "\n",
        "            hs.append(state)\n",
        "            ha.append(action)\n",
        "            hr.append(reward)\n",
        "\n",
        "            # se o histórico estiver completo com 'n' passos\n",
        "            # vai fazer uma atualização no valor Q do estado mais antigo\n",
        "            if len(hs) == nsteps:\n",
        "                if terminated:\n",
        "                    # para estados terminais\n",
        "                    V_next_state = 0\n",
        "                else:\n",
        "                    # para estados não-terminais -- valor da próxima ação (já escolhida)\n",
        "                    V_next_state = Q[next_state,next_action]\n",
        "\n",
        "                # delta = (estimativa usando a nova recompensa) - estimativa antiga\n",
        "                delta = ( sum(gamma_array * hr) + gamma_power_nstep * V_next_state ) - Q[hs[0],ha[0]]\n",
        "\n",
        "                # atualiza a Q-table para o par (estado,ação) de n passos atrás\n",
        "                Q[hs[0],ha[0]] += lr * delta\n",
        "\n",
        "            # preparação para avançar mais um passo\n",
        "            # lembrar que a ação a ser realizada já está escolhida\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "            # fim do laço por episódio\n",
        "\n",
        "        # ao fim do episódio, atualiza o Q dos estados que restaram no histórico\n",
        "\n",
        "        # é igual ao V_next_state, exceto em episódios muito curtos (com duração menor que \"nsteps\")\n",
        "        V_end_state = 0 if terminated else Q[next_state,next_action]\n",
        "\n",
        "        # inferior ao \"nsteps\" apenas em episódios muito curtos\n",
        "        steps_to_end = min(nsteps, len(hs))\n",
        "        for j in range(steps_to_end-1,0,-1):\n",
        "            hs.popleft()\n",
        "            ha.popleft()\n",
        "            hr.popleft()\n",
        "            delta = ( sum(gamma_array[0:j]*hr) + gamma_array[j]*V_end_state ) - Q[hs[0],ha[0]]\n",
        "            Q[hs[0],ha[0]] += lr * delta\n",
        "\n",
        "        sum_rewards_per_ep.append(sum_rewards)\n",
        "\n",
        "        # a cada 100 episódios, imprime informação sobre o progresso\n",
        "        if verbose and ((i+1) % 100 == 0):\n",
        "            avg_reward = np.mean(sum_rewards_per_ep[-100:])\n",
        "            print(f\"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "    return sum_rewards_per_ep, Q"
      ],
      "metadata": {
        "id": "SWzBXOc3Dslw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-xEwtye5J_r"
      },
      "source": [
        "## Funções de execução de teste e treinamento\n",
        "* vary_nsteps : Variação dos valores de passos\n",
        "* vary_LR : Variação dos valores de LR\n",
        "* vary_EPSILON: Variação dos valores de Epsilon\n",
        "* vary_GAMMA: Variação dos valores de gamma\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vary_nsteps(env, values_list:list,number_of_executions, **kargs):\n",
        "\n",
        "  #setup aplicação\n",
        "  EPISODES = kargs[\"EPISODES\"]\n",
        "  LR = kargs[\"LR\"]\n",
        "  GAMMA = kargs[\"GAMMA\"]\n",
        "  EPSILON = kargs[\"EPSILON\"]\n",
        "\n",
        "  #Escolher imprimir ou não\n",
        "  verbose = kargs[\"verbose\"]\n",
        "\n",
        "  #Estruturas de dados para armazenamento\n",
        "  dict_of_each_parameter = dict()\n",
        "\n",
        "  for nsteps in values_list:\n",
        "    # Treinar o modelo\n",
        "    dict_of_each_parameter[nsteps] = list()\n",
        "    if verbose:\n",
        "      print(f\"Treinando com {nsteps} passos\")\n",
        "    rewards_training, Q = run_nstep_sarsa_by_eps(env, EPISODES, nsteps=nsteps, lr=LR, gamma=GAMMA, epsilon=EPSILON, verbose=verbose)\n",
        "    for i in range(number_of_executions):\n",
        "      if verbose:\n",
        "        print(f\"Rodando experimento {i+1} de {nsteps} passos\")\n",
        "\n",
        "      rewards_test, _ = run_nstep_sarsa_by_eps(env, EPISODES, nsteps=nsteps, lr=LR, gamma=GAMMA, epsilon=EPSILON, verbose=verbose, Q=Q)\n",
        "      dict_of_each_parameter[nsteps].append(rewards_test)\n",
        "\n",
        "  return dict_of_each_parameter\n",
        "\n",
        "\n",
        "def vary_LR(env, values_list:list,number_of_executions, **kargs):\n",
        "\n",
        "  #setup aplicação\n",
        "  EPISODES = kargs[\"EPISODES\"]\n",
        "  GAMMA = kargs[\"GAMMA\"]\n",
        "  EPSILON = kargs[\"EPSILON\"]\n",
        "  #Escolher imprimir ou não\n",
        "  verbose = kargs[\"verbose\"]\n",
        "\n",
        "  #Estruturas de dados para armazenamento\n",
        "  dict_of_each_parameter = dict()\n",
        "\n",
        "  for lr in values_list:\n",
        "    # Treinar o modelo\n",
        "    dict_of_each_parameter[lr] = list()\n",
        "    if verbose:\n",
        "      print(f\"Treinando com {lr} passos\")\n",
        "    rewards_training, Q = run_nstep_sarsa_by_eps(env, EPISODES, nsteps=NSTEPS, lr=lr, gamma=GAMMA, epsilon=EPSILON, verbose=verbose)\n",
        "    for i in range(number_of_executions):\n",
        "      if verbose:\n",
        "        print(f\"Rodando experimento {i+1} de {lr} passos\")\n",
        "\n",
        "      rewards_test, _ = run_nstep_sarsa_by_eps(env, EPISODES, nsteps=NSTEPS, lr=lr, gamma=GAMMA, epsilon=EPSILON, verbose=verbose, Q=Q)\n",
        "      dict_of_each_parameter[lr].append(rewards_test)\n",
        "\n",
        "\n",
        "\n",
        "  return dict_of_each_parameter\n",
        "\n",
        "def vary_EPSILON(env, values_list:list,number_of_executions, **kargs):\n",
        "\n",
        "  #setup aplicação\n",
        "  EPISODES = kargs[\"EPISODES\"]\n",
        "  GAMMA = kargs[\"GAMMA\"]\n",
        "  LR = kargs[\"LR\"]\n",
        "  #Escolher imprimir ou não\n",
        "  verbose = kargs[\"verbose\"]\n",
        "\n",
        "  #Estruturas de dados para armazenamento\n",
        "  dict_of_each_parameter = dict()\n",
        "\n",
        "  for epsilon in values_list:\n",
        "    # Treinar o modelo\n",
        "    dict_of_each_parameter[epsilon] = list()\n",
        "    if verbose:\n",
        "      print(f\"Treinando com {epsilon} passos\")\n",
        "    rewards_training, Q = run_nstep_sarsa_by_eps(env, EPISODES, nsteps=NSTEPS, lr=LR, gamma=GAMMA, epsilon=epsilon, verbose=verbose)\n",
        "    for i in range(number_of_executions):\n",
        "      if verbose:\n",
        "        print(f\"Rodando experimento {i+1} de {epsilon} passos\")\n",
        "\n",
        "      rewards_test, _ = run_nstep_sarsa_by_eps(env, EPISODES, nsteps=NSTEPS, lr=LR, gamma=GAMMA, epsilon=epsilon, verbose=verbose, Q=Q)\n",
        "      dict_of_each_parameter[epsilon].append(rewards_test)\n",
        "\n",
        "\n",
        "\n",
        "  return dict_of_each_parameter\n",
        "\n",
        "\n",
        "def vary_GAMMA(env, values_list:list,number_of_executions, **kargs):\n",
        "  #setup aplicação\n",
        "  EPISODES = kargs[\"EPISODES\"]\n",
        "  LR = kargs[\"LR\"]\n",
        "  EPSILON = kargs[\"EPSILON\"]\n",
        "  #Escolher imprimir ou não\n",
        "  verbose = kargs[\"verbose\"]\n",
        "\n",
        "  #Estruturas de dados para armazenamento\n",
        "  dict_of_each_parameter = dict()\n",
        "\n",
        "  for gamma in values_list:\n",
        "    # Treinar o modelo\n",
        "    dict_of_each_parameter[gamma] = list()\n",
        "    if verbose:\n",
        "      print(f\"Treinando com {gamma} passos\")\n",
        "    rewards_training, Q = run_nstep_sarsa_by_eps(env, EPISODES, nsteps=NSTEPS, lr=LR, gamma=gamma, epsilon=EPSILON, verbose=verbose)\n",
        "    for i in range(number_of_executions):\n",
        "      if verbose:\n",
        "        print(f\"Rodando experimento {i+1} de {gamma} passos\")\n",
        "\n",
        "      rewards_test, _ = run_nstep_sarsa_by_eps(env, EPISODES, nsteps=NSTEPS, lr=LR, gamma=gamma, epsilon=EPSILON, verbose=verbose, Q=Q)\n",
        "      dict_of_each_parameter[gamma].append(rewards_test)\n",
        "\n",
        "\n",
        "\n",
        "  return dict_of_each_parameter\n"
      ],
      "metadata": {
        "id": "oobDQA7KCuB5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment(env, env_name:str):\n",
        "  rewards_gamma = vary_GAMMA(env, GAMMA_RAND, ITERATIONS_TO_CALCULATE_MEAN, LR=LR, EPSILON=EPSILON, verbose=True, EPISODES=EPISODES, NSTEPS=NSTEPS)\n",
        "  rewards_epsilon = vary_EPSILON(env, EPSILON_RAND, ITERATIONS_TO_CALCULATE_MEAN, LR=LR,GAMMA=GAMMA, verbose=True, EPISODES=EPISODES, NSTEPS=NSTEPS)\n",
        "  rewards_lr = vary_LR(env, LR_RAND, ITERATIONS_TO_CALCULATE_MEAN, GAMMA=GAMMA, EPSILON=EPSILON, verbose=True, EPISODES=EPISODES, NSTEPS=NSTEPS)\n",
        "  rewards_nsteps = vary_nsteps(env, NSTEPS_RAND, ITERATIONS_TO_CALCULATE_MEAN, LR=LR, GAMMA=GAMMA, EPSILON=EPSILON, verbose=False, EPISODES=EPISODES)\n",
        "\n"
      ],
      "metadata": {
        "id": "0Y8kuJgPrXY5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execuções preliminares"
      ],
      "metadata": {
        "id": "toOhSzNmtC_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4x5E-zx85J_u",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# para ambientes gymnasium\n",
        "#ENV_NAME, r_max = \"Taxi-v3\", 10\n",
        "# ENV_NAME, r_max = \"CliffWalking-v0\", 0\n",
        "# ENV_NAME, r_max = \"FrozenLake-v1\", 0\n",
        "ENV_NAME, r_max = \"RaceTrack-v0\", 0\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "\n",
        "ITERATIONS_TO_CALCULATE_MEAN = 2\n",
        "EPISODES = 1_0\n",
        "EPISODES_TEST = 1_0\n",
        "\n",
        "# Hyperparameters quando eles não são o foco do estudo\n",
        "NSTEPS = 3\n",
        "LR = 0.1\n",
        "GAMMA = 0.95\n",
        "EPSILON = 0.1\n",
        "\n",
        "# Listas de Hyperparameters para que sejam executados os testes.\n",
        "NSTEPS_RAND = [2**x for x in range(10)]\n",
        "LR_RAND = np.random.uniform(low=0.1, high=1.0, size=(10,))\n",
        "EPSILON_RAND = np.random.uniform(low=0.1, high=1.0, size=(10,))\n",
        "GAMMA_RAND = np.random.uniform(low=0.1, high=0.95, size=(10,))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_gamma = vary_GAMMA(env, GAMMA_RAND, ITERATIONS_TO_CALCULATE_MEAN, LR=LR, EPSILON=EPSILON, verbose=True, EPISODES=EPISODES, NSTEPS=NSTEPS)"
      ],
      "metadata": {
        "id": "b4P_aSdcak3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in rewards_gamma.items():\n",
        "  print(f\"#{key}\\t Média:{np.mean(value)}\\t\\tDesvio Padrão: {np.std(value)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "M_c7YCQDbWSC",
        "outputId": "0fc2f960-1095-4545-f6c0-0578c788d516"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#0.10691801259342172\t Média:-112.35\t\tDesvio Padrão: 44.39850785780982\n",
            "#0.7567121610904911\t Média:-126.25\t\tDesvio Padrão: 40.44363361519338\n",
            "#0.2701062105859774\t Média:-120.2\t\tDesvio Padrão: 36.289943510565024\n",
            "#0.5880488108372625\t Média:-142.0\t\tDesvio Padrão: 18.365728953678914\n",
            "#0.8191887518623385\t Média:-126.0\t\tDesvio Padrão: 40.65587288449235\n",
            "#0.5698948091687034\t Média:-110.6\t\tDesvio Padrão: 42.687703147393634\n",
            "#0.6759190945123904\t Média:-110.35\t\tDesvio Padrão: 42.490322427583436\n",
            "#0.11335524548805287\t Média:-127.25\t\tDesvio Padrão: 38.39905597797946\n",
            "#0.7456100466078105\t Média:-134.05\t\tDesvio Padrão: 26.369442542458117\n",
            "#0.635376612995947\t Média:-122.2\t\tDesvio Padrão: 42.011427016943856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_nsteps = vary_nsteps(env, NSTEPS_RAND, ITERATIONS_TO_CALCULATE_MEAN, LR=LR, GAMMA=GAMMA, EPSILON=EPSILON, verbose=False, EPISODES=EPISODES)"
      ],
      "metadata": {
        "id": "0fcFS_uX8P_U"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in rewards_nsteps.items():\n",
        "  print(f\"#{key}\\t Média:{np.mean(value)}\\t\\tDesvio Padrão: {np.std(value)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ROdG9VThRSs8",
        "outputId": "8260b770-f04e-4a7d-9151-92d667a09e8b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#1\t Média:-118.8\t\tDesvio Padrão: 44.041571270789156\n",
            "#2\t Média:-93.85\t\tDesvio Padrão: 42.86639126401942\n",
            "#4\t Média:-119.3\t\tDesvio Padrão: 43.1\n",
            "#8\t Média:-134.1\t\tDesvio Padrão: 35.60182579587738\n",
            "#16\t Média:-138.35\t\tDesvio Padrão: 26.025516325329647\n",
            "#32\t Média:-122.4\t\tDesvio Padrão: 45.52845264227635\n",
            "#64\t Média:-142.3\t\tDesvio Padrão: 23.46294951620533\n",
            "#128\t Média:-140.15\t\tDesvio Padrão: 28.282989587382726\n",
            "#256\t Média:-135.15\t\tDesvio Padrão: 26.981984730556793\n",
            "#512\t Média:-133.2\t\tDesvio Padrão: 34.52622191899948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_lr = vary_LR(env, LR_RAND, ITERATIONS_TO_CALCULATE_MEAN, GAMMA=GAMMA, EPSILON=EPSILON, verbose=True, EPISODES=EPISODES, NSTEPS=NSTEPS)"
      ],
      "metadata": {
        "id": "rPJ3RGMM8ZJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in rewards_lr.items():\n",
        "  print(f\"#{key}\\t Média:{np.mean(value)}\\t\\tDesvio Padrão: {np.std(value)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cLDX_XE_6KT7",
        "outputId": "b8e4a9f1-7073-4dff-dc06-206a88ec38c1"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#0.7414197005512627\t Média:-124.65\t\tDesvio Padrão: 34.84146236885014\n",
            "#0.980994323727853\t Média:-126.45\t\tDesvio Padrão: 34.43904034667633\n",
            "#0.39459496814603645\t Média:-140.0\t\tDesvio Padrão: 20.24598725673806\n",
            "#0.9514998040758166\t Média:-129.45\t\tDesvio Padrão: 33.7245830218848\n",
            "#0.6917801118404305\t Média:-128.3\t\tDesvio Padrão: 34.17908717329941\n",
            "#0.41153183218705347\t Média:-116.15\t\tDesvio Padrão: 41.27502271350072\n",
            "#0.6895801178068364\t Média:-110.45\t\tDesvio Padrão: 39.49110659376362\n",
            "#0.13291738840305725\t Média:-135.6\t\tDesvio Padrão: 28.24606167238187\n",
            "#0.5624237690423813\t Média:-130.55\t\tDesvio Padrão: 26.53389341954927\n",
            "#0.41995191781471664\t Média:-124.65\t\tDesvio Padrão: 40.50589463275685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_epsilon = vary_EPSILON(env, EPSILON_RAND, ITERATIONS_TO_CALCULATE_MEAN, LR=LR,GAMMA=GAMMA, verbose=True, EPISODES=EPISODES, NSTEPS=NSTEPS)"
      ],
      "metadata": {
        "id": "A0JdepAi8dC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in rewards_epsilon.items():\n",
        "  print(f\"#{key}\\t Média:{np.mean(value)}\\t\\tDesvio Padrão: {np.std(value)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9s6Y7RS-7hXk",
        "outputId": "7bd41299-c86a-4c29-f3b8-f708800b3a8e"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#0.17406257093718114\t Média:-127.45\t\tDesvio Padrão: 39.30582017971384\n",
            "#0.48979584093646134\t Média:-110.55\t\tDesvio Padrão: 48.08999376169641\n",
            "#0.6053902548245806\t Média:-144.05\t\tDesvio Padrão: 11.355505272774083\n",
            "#0.21347623688434325\t Média:-123.45\t\tDesvio Padrão: 31.853531986264883\n",
            "#0.7926663288199893\t Média:-118.05\t\tDesvio Padrão: 43.23710790513168\n",
            "#0.9298904795823542\t Média:-121.65\t\tDesvio Padrão: 36.904301917256205\n",
            "#0.24862326088860406\t Média:-104.8\t\tDesvio Padrão: 42.9902314485512\n",
            "#0.6987369822818559\t Média:-123.95\t\tDesvio Padrão: 40.54192274670751\n",
            "#0.595112916305213\t Média:-131.3\t\tDesvio Padrão: 30.990482409927083\n",
            "#0.6343862394641873\t Média:-114.65\t\tDesvio Padrão: 44.386118325440435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fim do Projeto\n"
      ],
      "metadata": {
        "id": "xOUudv7zZD3G"
      }
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "rl23",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}